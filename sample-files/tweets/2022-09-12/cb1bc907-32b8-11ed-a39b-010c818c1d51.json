{"text": "RT @ParmidaBeigi: Amazon's 20B language model has fewer than 1/8 the number of parameters of GPT-3, yet outperforms it in several NLP tasks…", "user": "AmazonScience", "created_at": "2022-09-12 16:33:45.000000", "source": "Twitter for iPhone", "platform": "Twitter", "text_clean": "rt @parmidabeigi: amazon's 20b language model has fewer than 1/8 the number of parameters of gpt-3, yet outperforms it in several nlp tasks…", "category_type": "machine learning", "category_type_score": 0.6919503211975098, "category_type_model_result": "{\"machine learning\": 0.6919503211975098, \"compute\": 0.18625091016292572, \"security\": 0.06642784178256989, \"database\": 0.02945137396454811, \"storage\": 0.025919586420059204}", "model": "SageMakerEndpoint-vRHkKJsUqRTs", "sentiment": "NEUTRAL", "timestamp": "2022-09-12 16:34:38.130057", "count": 1}